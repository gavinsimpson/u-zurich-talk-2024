<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Avoiding linearity</title>
    <meta charset="utf-8" />
    <meta name="author" content="Gavin Simpson" />
    <meta name="date" content="2024-12-17" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" type="text/css" />
    <link rel="stylesheet" href="slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: inverse, middle, left, my-title-slide, title-slide

.title[
# Avoiding linearity
]
.author[
### Gavin Simpson
]
.institute[
### Aarhus University
]
.date[
### December 17, 2024
]

---


# Slides &amp; code



* GitHub repo: [bit.ly/uz-gams](https://bit.ly/uz-gams)
* Slides: [bit.ly/uz-gam-talk](https://bit.ly/uz-gam-talk)

.center[

&lt;img src="resources/bit.ly_uz-gam-talk.png" width="45%" style="display: block; margin: auto;" /&gt;

]



---
class: inverse middle center subsection

# Nonlinearity

---

# HadCRUT5 time series

&lt;img src="index_files/figure-html/hadcrut-temp-example-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

???

Hadley Centre Global temperature record ensemble

Data are anomalies, relative to 1961-1990 mean

How would you model the trend in these data?

---

# Linear Models

`$$y_i \sim \mathcal{N}(\mu_i, \sigma^2)$$`

`$$\mu_i = \beta_0 + \beta_1 \mathtt{year}_{i} + \beta_2 \mathtt{year}^2_{1i} + \cdots + \beta_j \mathtt{year}^j_{i}$$`

---

# Polynomials perhaps&amp;hellip;

&lt;img src="index_files/figure-html/hadcrut-temp-polynomial-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

---

# Polynomials perhaps&amp;hellip;

We can keep on adding ever more powers of `\(\boldsymbol{x}\)` to the model &amp;mdash;
model selection problem

**Runge phenomenon** &amp;mdash; oscillations at the edges of an interval &amp;mdash;
means simply moving to higher-order polynomials doesn't always improve accuracy

---
class: inverse middle center subsection

# GAMs offer a solution

---

# Generalized Additive Models

&lt;br /&gt;

![](resources/tradeoff-slider.png)

.references[Source: [GAMs in R by Noam Ross](https://noamross.github.io/gams-in-r-course/)]

???

GAMs are an intermediate-complexity model

* can learn from data without needing to be informed by the user
* remain interpretable because we can visualize the fitted features

---

# How is a GAM different?

`$$\begin{align*}
y_i &amp;\sim \mathcal{D}(\mu_i, \theta) \\ 
\mathbb{E}(y_i) &amp;= \mu_i = g(\eta_i)^{-1}
\end{align*}$$`

We model the mean of data as a sum of linear terms:

`$$\eta_i = \beta_0 +\sum_j \color{red}{ \beta_j x_{ji}}$$`

A GAM is a sum of _smooth functions_ or _smooths_

`$$\eta_i = \beta_0 + \sum_j \color{red}{f_j(x_{ji})}$$`

---

# Fitting a GAM in R

```r
model &lt;- gam(y ~ s(x1) + s(x2) + te(x3, x4), # formuala describing model
             data = my_data_frame,           # your data
             method = "REML",                # or "ML"
             family = gaussian)              # or something more exotic
```

`s()` terms are smooths of one or more variables

`te()` terms are the smooth equivalent of *main effects + interactions*

`$$\eta_i = \beta_0 + f(\mathtt{Year}_i)$$`

```r
library(mgcv)
gam(Temperature ~ s(Year, k = 10), data = gtemp, method = 'REML')
```

---

# Fitted GAM

&lt;img src="index_files/figure-html/hadcrtemp-plot-gam-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

---

# How did `gam()` *know* what line to fit?

&lt;img src="index_files/figure-html/hadcrtemp-plot-gam-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

---



# Wiggly things

.center[![](resources/spline-anim.gif)]

???

GAMs use splines to represent the non-linear relationships between covariates,
here `x`, and the response variable on the `y` axis.

---

# Basis expansions

In the polynomial models we used a polynomial basis expansion of `\(\boldsymbol{x}\)`

* `\(\boldsymbol{x}^0 = \boldsymbol{1}\)` &amp;mdash; the model constant term
* `\(\boldsymbol{x}^1 = \boldsymbol{x}\)` &amp;mdash; linear term
* `\(\boldsymbol{x}^2\)`
* `\(\boldsymbol{x}^3\)`
* &amp;hellip;

---

# Splines

Splines are *functions* composed of simpler functions

Simpler functions are *basis functions* &amp; the set of basis functions is a *basis*

When we model using splines, each basis function `\(b_k\)` has a coefficient `\(\beta_k\)`

Resultant spline is a the sum of these weighted basis functions, evaluated at the values of `\(x\)`

`$$f(x) = \sum_{k = 1}^K \beta_k b_k(x)$$`

---

# Splines formed from basis functions

&lt;img src="index_files/figure-html/basis-functions-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

???

Splines are built up from basis functions

Here I'm showing a cubic regression spline basis with 10 knots/functions

We weight each basis function to get a spline. Here all the basisi functions have the same weight so they would fit a horizontal line

---

# Weight basis functions &amp;#8680; spline



.center[![](resources/basis-fun-anim.gif)]

???

But if we choose different weights we get more wiggly spline

Each of the splines I showed you earlier are all generated from the same basis functions but using different weights

---

# How do GAMs learn from data?

&lt;img src="index_files/figure-html/example-data-figure-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

???

How does this help us learn from data?

Here I'm showing a simulated data set, where the data are drawn from the orange functions, with noise. We want to learn the orange function from the data

---

# Maximise penalised log-likelihood &amp;#8680; &amp;beta;



.center[![](resources/gam-crs-animation.gif)]

???

Fitting a GAM involves finding the weights for the basis functions that produce a spline that fits the data best, subject to some constraints


---
class: inverse middle center subsection

# Avoid overfitting our sample

---
class: inverse center middle large-subsection

# How wiggly?

$$
\int_{\mathbb{R}} [f^{\prime\prime}]^2 dx = \boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}
$$

---
class: inverse center middle large-subsection

# Penalised fit

$$
\mathcal{L}_p(\boldsymbol{\beta}) = \mathcal{L}(\boldsymbol{\beta}) - \frac{1}{2} \lambda\boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}
$$

---

# Making wiggliness matter

`\(\boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}\)` measures **wiggliness**

(log) likelihood measures closeness to the data

We use a **smoothing parameter** `\(\lambda\)` to define the trade-off, to find
the spline coefficients `\(b_k\)` that maximize the **penalized** log-likelihood

`$$\mathcal{L}_p = \log(\text{Likelihood})  - \lambda \boldsymbol{\beta}^{\mathsf{T}}\mathbf{S}\boldsymbol{\beta}$$`

---

# HadCRUT5 time series

&lt;img src="index_files/figure-html/hadcrut-temp-penalty-1.svg" width="100%" style="display: block; margin: auto;" /&gt;

---
class: inverse subsection center middle

# Examples

---
class: inverse subsection center middle

# Trends

---

# Two case studies

1. Trends in biodiversity data, and

2. Shifting seasonality in cases of Lyme borreliosis

---
class: inverse middle center subsection

# Biodiversity trends

---

# Biodiversity losses

.center[![:scale 75%](./resources/living-planet-index-by-region.svg)
]

???

Many of us will be familiar with trends like this

Global biodiversity is in trouble

Wherever we look we see declining populations

Living Plant Index is an average measure of change (so some time series in a region could be increasing even if overall we have a decline in LPI)

---

# Biodiversity losses..?

.center[![:scale 100%](./resources/vellend-et-al-2013.png)
]

Vellend *et al* (2013) *PNAS*

.center[![:scale 100%](./resources/dornelas-et-al-2014.png)
]

Dornelas *et al* 2014 *Science*

???

But studies showing declining biodiversity are not without issue

There has been a decade or more long discussion about the extent to which diversity is declining

---

# Biodiversity losses..?

.center[![:scale 100%](./resources/gonzalez-et-al-2016.png)
]

Gonzalez *et al* (2016) *Ecology*

.center[![:scale 100%](./resources/vellend-et-al-2017.png)
]

Vellend *et al* 2017 *Ecology*

---

# Biodiversity gains &amp; losses

.center[![:scale 75%](./resources/dornelas-et-al-2019.png)
]

Dornelas *et al* (2019) *Ecology Letters*

In part disagreement relates to *population* versus *assemblage* level metrics

Partly due to how the data are analysed

---

# Linear trends

Majority of these large-scale syntheses use linear trend models

.center[![:scale 75%](./resources/dornelas-et-al-2014-fig-2.jpeg)
]

Fig 2 from Dornelas *et al* (2014) *Science*

`\(y_i\)` sometimes transformed

---

# Seibold et al 2019

Analysed 150 grassland and 140 forest sites across 3 regions of Germany 2008&amp;ndash;2017

.center[![:scale 75%](./resources/seibold-et-al-2019.png)
]

Seibold *et al* (2019) *Nature*

???

As an example of why using linear trends might be problematic, I will use this recent example from Seibold et al

---

# Declining arthopod biomass, abundance, and richness

.center[![:scale 60%](./resources/seibold-et-al-fig-1.svg)
]

Seibold *et al* (2019) *Nature*

---

# Declining arthopod biomass, abundance, and richness

* Grasslands

    * Biomass &amp;#8595; 67%

    * Abundance &amp;#8595; 78%

    * Richness &amp;#8595; 36%

* Forests (30 site subset)

    * Biomass &amp;#8595; 41%

    * Richness &amp;#8595; 36%

---

# Good and bad years

.center[
![:scale 60%](./resources/fournier-et-al-2019.svg)
]

Fournier *et al* (2019) *Conservation Biology* [doi: 10.1111/cobi.13371](https://doi.org/10.1111/cobi.13371)

---

# Good and bad years

Daskalova *et al* (2021) attempted to account for this effect using random effects of `Year`

.center[
![:scale 75%](./resources/daskalova-et-al-2021-fig-1.svg)
]

Daskalova *et al* (2021) *Insect Conservation &amp; Diversity*

---

# Generalized Additive Models

Random effects of `Year` are one form of common trend

Year-to-year variation about some overall mean

--

Random effects are a special case of a penalized spline

---

# Arthropod abundance

.center[![:scale 90%](./resources/arthropod-abundance.svg)
]

---

# Arthropod abundance

Negative binomial GAM fitted to single site (AEG1)

.center[![:scale 70%](./resources/arthropod-abundance-site-aeg1-negbin-gam.svg)
]

---

# Grassland arthropod abundance

Our example:

* Abundance of all identified Arthropods

* Grassland sites only

* Models

    * Linear global and site-specific trends (random slopes &amp; intercepts)
    * Smooth trend of `year` plus `site` random intercepts
    * Regional smooths of `year` plus `site` random intercepts
    * Regional smooths of `year`, `site` specific *random* smooths
    * Regional smooths of `year`, year-to-year effects, `site` specific *random* smooths

---

# Random smooths?

* Random intercepts
* Random slopes
* Random smooths?

Random smooths yield a separate smooth for each subject

Share a wiggliness penalty (plus `\(\lambda\)` for intercepts &amp; slopes)

But individual trends can have different shape

---

# Random smooth model

`site` specific *random* smooths

.center[![:scale 70%](./resources/arthropod-abundance-m3.svg)
]

Pedersen _et al_ (2019) [PeerJ **7**:e6876](https://doi.org/10.7717/peerj.6876)

---

# Model A

Regional smooths of `year`, `site` specific *random* smooths

.center[![:scale 80%](./resources/arthropod-abundance-m4.svg)
]

---

# Model B

Regional smooths of `year`, year-to-year effects, `site` specific *random* smooths

.center[![:scale 80%](./resources/arthropod-abundance-m5.svg)
]

---

# Results

* Model A (without Y2Y effects): AIC 15122 (255.7 EDF)
* Model B (with Y2Y effects): AIC 15123 (254.2 EDF)
* Equivalent mixed linear trend model (with Y2Y effects): AIC 15468 (111.8 EDF)

Use year-to-year effects or not?

--

(For these data) **A** and **B** are different ways to decompose the temporal effects

* Without Y2Y &amp;#8594;	wigglier regional smooths
* With Y2Y &amp;#8594; less wiggly regional smooths

???

Even with relatively simple data, GAMs yield much better fit than linear equiv.

---

# Nothing new under the sun

Fewster, Buckland, *et al* (2000) *Ecology*

(except Simon Wood's excellent {mgcv} software &amp; ability to fit random smooths)

Jonas Knape (2016, *J. App. Ecol.*)

---

# Summary

How we model trends in biodiversity data can change our view of losses and gains

Using "smooth" trends gives much better fit for the arthropod data here

Y2Y effects and / or smooth trends represent different decompositions of the *temporal* effect

Non-linear methods needed if we want to assess changes in rates of change

---
class: inverse subsection center middle

# Lyme borreliosis



---

# Lyme borreliosis

.row[

.col-9[
Lyme disease is a zoonotic infection

Caused by bacteria (spirochetes) in the *Borrelia burgdorferi* sensu lato complex

Transmitted by infected tick larvae &amp; nymphs in the genus *Ixodes* during blood meals

Common vector-borne disease in N hemisphere

Reported cases of Lyme borreliosis have increased in recent decades
]

.col-3[

.center[
&lt;img src="resources/Adult_deer_tick.jpg" width="1763" style="display: block; margin: auto;" /&gt;
]

.smaller[Adult deer tick, *Ixodes scapularis*

(Source: Scott Bauer)
]

]

]
???

Increased in both number and geographical range

---

# Tick-borne disease &amp; climate

.center[
&lt;img src="resources/gilbert-tick-zoonoses-schematic.png" width="90%" style="display: block; margin: auto;" /&gt;
]

.smaller[source: Gilbert (2021) *Annu. Rev. Entomol.* [10.1146/annurev-ento-052720-094533](https://doi.org/10.1146/annurev-ento-052720-094533)
]

???

Figure caption from Gilbert (2021)

Schematic diagram showing how climate change can affect ticks directly (by changing oviposition, development, mortality rates, and activity) and indirectly (by changing habitat and host species and abundance). Climate change can, in turn, affect tick-borne pathogen infection risk in humans by directly affecting human behavior (e.g., outdoor recreation) and indirectly affecting pathogen transmission rates and prevalence via hosts and ticks. The relative importance of each pathway is challenging to ascertain.

---

# Tick-borne disease &amp; climate

Aim of Goren *et al* (2023) was to study

1. the incidence, and

2. the seasonal timing

of Lyme borreliosis at the northern limit of the range in Europe

---

# Cases of Lyme borreliosis in Norway

&lt;img src="index_files/figure-html/lyme-cases-plot-1.svg" width="95%" style="display: block; margin: auto;" /&gt;
.small[
Data: Goren *et al* (2023) *Proc. Biol. Sci.* [10.1098/rspb.2022.2420](https://doi.org/10.1098/rspb.2022.2420)
]

---

# Climate change in Norway

Compared to 1979&amp;ndash;2008 reference period

Annual mean temperature increased by ~ 0.5&amp;#8451;

Winter temperatures increase by ~1&amp;#8451;

Growing season increased by 1&amp;ndash;2 weeks nationally

Annual precipitation increased by ~3% per decade

---

# GAM for seasonal data

Assume weekly cases are distributed Negative binomial

$$
\texttt{cases}_i \sim \mathcal{NB}(\mu_i, \theta)
$$

Lots of ways to decompose these data; settled on

.small[
$$
\log (\mathbb{E}(\texttt{cases}_i) ) = \beta_0 + f(\texttt{week}_i , \texttt{year}_i) + \log(\texttt{population}_i)
$$
]

where

$$
f(\texttt{week}_i , \texttt{year}_i)
$$

is a tensor product smooth (main effects plus interaction)

---

# GAM for seasonal data

Model code looks like:


``` r
m &lt;- bam(cases ~ s(week, bs = "cc", k = 20) +
          s(year, k = 25) +
          ti(week, year, bs = c("cc", "tp"), k = c(20, 20)) +
          offset(log(pop)),
    data = lyme,
    family = nb(),
    method = "fREML",
    knots = list(week = c(0.5, 52.5)),
    discrete = TRUE,
    nthreads = 4)
```

Using lots of *mgcv* tricks to fit the model *fast* as some decompositions take a long time to fit with `gam()`

Decomposed tensor product into main smooth effects plus a pure smooth interaction

---

# Estimated smooth functions

&lt;img src="index_files/figure-html/lyme-draw-1.svg" width="95%" style="display: block; margin: auto;" /&gt;

---

# Estimated seasonal curves

&lt;img src="index_files/figure-html/lyme-plot-fitted-curves-1.svg" width="95%" style="display: block; margin: auto;" /&gt;

---

# How has timing of peak changed?

Easy &amp;mdash; using data in previous figure, find week in each year where `\(\widehat{\texttt{cases}}_{ij}\)` is maximal

--

Harder &amp;mdash; account for the uncertainty in the model &amp; put an uncertainty band on `\(\widehat{\texttt{peak}}_{ij}\)`

--

GAMs are an *empirical Bayesian* model so we can simulate from the posterior distribution of the model to get answers to questions like this

---

# How has timing of peak changed?

*gratia* 📦 has functions to make doing this easy(-ish)


``` r
ds2 &lt;- data_slice(m, week = unique(week), year = unique(year),
  pop = log(100000))

ds2 &lt;- ds2 |&gt;
  mutate(.row = row_number()) |&gt;
  relocate(.row, .before = 1L)

fs2 &lt;- fitted_samples(m, data = ds2, n = 10000, seed = 42, scale = "response")

fs2 &lt;- fs2 |&gt;
  left_join(ds2, by = join_by(.row)) |&gt;
  select(-pop)

peak_week &lt;- fs2 |&gt;
  group_by(year, .draw) |&gt;
  slice_max(order_by = .fitted, n = 1) |&gt;
  group_by(year) |&gt;
  summarise(peak = quantile(week, prob = 0.5),
    lower_ci = quantile(week, prob = 0.055),
    upper_ci = quantile(week, prob = 0.945))
```

---

# How has timing of peak changed?

&lt;img src="index_files/figure-html/lyme-plot-peak-week-1.svg" width="95%" style="display: block; margin: auto;" /&gt;

---

# Summary

GAMs offer a flexible way to model (decompose) temporal variation in ecological time series

Can be adapted in many ways

* space
* space &amp; time
* covariate effects
* model distributions or quantiles
* handle very large data

Posterior inference allows us to do many useful things (easily &amp;mdash; more easily)

---

# Thank you &amp; More 📷

Mail: gavin@anivet.au.dk

[GitHub - bit.ly/uz-gams](https://bit.ly/uz-gams)

.center[

&lt;img src="resources/bit.ly_uz-gam-talk.png" width="45%" style="display: block; margin: auto;" /&gt;
]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
